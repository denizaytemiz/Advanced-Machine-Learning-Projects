{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4Ip4Tz6vjkn"
   },
   "source": [
    "# Programming Assignment 3: K-means clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQrd0XNmvjkp"
   },
   "source": [
    "In this programming assignment, you will be implementing the k-means clustering algorithm, which is a popular unsupervised machine learning technique used to segment data into similar clusters. Unlike supervised learning, k-means clustering does not require pre-existing labels for the data.\n",
    "\n",
    "In this algorithm, the data is partitioned into k clusters, where k is a pre-defined value chosen by the user. The algorithm then iteratively assigns data points to their nearest cluster center, and updates the cluster centers based on the mean of the data points assigned to that cluster. This process continues until the cluster centers no longer change or a maximum number of iterations is reached.\n",
    "\n",
    "The goal of k-means clustering is to group together similar data points based on their distance from each other, while minimizing the distance between the data points within a cluster. The resulting clusters can be used to gain insights into the underlying structure of the data or to identify anomalous data points that do not fit into any cluster.\n",
    "\n",
    "To perform k-means clustering, you will be using the household_power_consumption_hourly.csv dataset, which contains hourly measurements of power consumption in a household. The notebook containing your implementation should be located in the same folder as the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iukj6q3wvjkq"
   },
   "source": [
    "## 1. Let's generate toy dataset to visualize the k means clustering in action:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8-Yq9ZQvjkr"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset with 2000 samples and 5 clusters\n",
    "X, y = make_blobs(n_samples=2000, centers=5, n_features=2, random_state=20)\n",
    "X = X* [1,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt9vXmAjUbNp"
   },
   "source": [
    "**Task 1**: (5 points)\n",
    "\n",
    "Please answer the following questions:\n",
    "1. Often, it is advisable to normalize the data (i.e., scale the feature) before feeding it into the clustering algorithm. What are the reasons for doing so? (Hint: You can examine the data before and after the feature scaling step below.)\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "tGGRV0a-wXUG",
    "outputId": "9348b788-9a33-492a-c700-c8084b362247"
   },
   "outputs": [],
   "source": [
    "# Feature scaling: Scale the data using minmax scaler\n",
    "sc = MinMaxScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1]))\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "df.plot(ax=ax, kind='scatter', x='x', y='y')\n",
    "plt.xlabel('X_1')\n",
    "plt.ylabel('X_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b7jMghv0mvY"
   },
   "source": [
    "**Task 2**: (15 points)\n",
    "\n",
    "Complete the algorithm code for k-means clustering that is implemented from scratch, without the use of any existing library implementations. Please follow the provided comments for guidance in writing this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAYfkWhaysAW"
   },
   "outputs": [],
   "source": [
    "# k_means_clustering method should return the final cluster centers and final data labels\n",
    "def k_means_clustering(X, k, initial_centroids, dist_metric='Euclidian', max_iter=20):\n",
    "  '''\n",
    "  Perform k-means clustering on the input data X.\n",
    "  Parameters:\n",
    "    X: input data, numpy array of shape (number of samples, number of features)\n",
    "    k: number of clusters\n",
    "    initial_centroids: a numpy array of shape (k, number of features) with initial values of the centroids specified by the user\n",
    "    dist_metric: distance metric to use for clustering, default is 'Euclidian'\n",
    "    max_iter: maximum number of iterations for the algorithm, default is 10\n",
    "  Returns:\n",
    "    new_centroids: a numpy array of shape (k, number of features) with updated centroids\n",
    "    labels: a numpy array of shape (number of samples, ) with cluster label ranging from 0 to k-1 for each input data point\n",
    "  '''\n",
    "\n",
    "  new_centroids = initial_centroids\n",
    "  old_centroids = initial_centroids\n",
    "  lables = []\n",
    "  for i in range(max_iter):\n",
    "\n",
    "    #STUDENT CODE HERE:\n",
    "    # Assign each data point to the closest centroid based on dis_metric and populate labels for each data points\n",
    "\n",
    "\n",
    "\n",
    "    #Update centroids based on new cluster assignments, and store the old centroids before assigning new values.\n",
    "\n",
    "    # Stop if the centroids haven't moved (i.e break if updated centroids are same as old centroids)\n",
    "\n",
    "  # Assign each data point to the closest centroid based on dis_metric and populate labels for each data points\n",
    "\n",
    "\n",
    "  #STUDENT CODE ENDS HERE\n",
    "\n",
    "  return new_centroids, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryEfyRKDJWYD"
   },
   "source": [
    "To evaluate the quality of our clustering results, we will use the Silhouette score, which measures the similarity of each data point to its own cluster compared to other clusters. This score is calculated by assigning a value to each data point based on its proximity to other data points within its cluster and its distance from data points in neighboring clusters. The Silhouette score can be calculated as \n",
    "\n",
    "Silhouette Score = (b-a)/max(a,b)\n",
    "\n",
    "where \n",
    "a= average intra-cluster distance i.e the average distance between each point within a cluster.\n",
    "\n",
    "b= average inter-cluster distance i.e the average distance between all clusters.\n",
    "\n",
    "**Task 3**: (Bonus 10 points) \n",
    "\n",
    "Below is an implementation of silhouette_score `cal_sil_score_my`. Check if this implementation gives the same value as the one provided by sklearn `cal_sil_score`. If they are not the same, see if you can debug the code. Provide your evaluation plots. \n",
    "\n",
    "For the rest, we will use `cal_sil_score` to be consistent with the rest of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rujKtVt4HCyu"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "def cal_sil_score(X, labels):\n",
    "\n",
    "    score= silhouette_score(X, labels)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhWXPNAKSnZZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "# from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "def cal_sil_score_my(X, labels):\n",
    "    '''\n",
    "    Calculate the Silhouette score for a given set of data points and cluster labels.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy array): A numpy array of shape (n_samples, n_features) representing the data points\n",
    "    labels (numpy array): A numpy array of shape (n_samples,) representing the cluster labels\n",
    "    \n",
    "    Returns:\n",
    "    silhouette_score (float): The Silhouette score for the given data points and cluster labels\n",
    "    '''\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    distances = pairwise_distances(X)\n",
    "    silhouettes = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Calculate the average distance from the current data point to all other points in its cluster\n",
    "        intra_cluster_dist = np.mean(distances[i][labels == labels[i]])\n",
    "        \n",
    "        # Calculate the average distance from the current data point to all points in the nearest neighboring cluster\n",
    "        other_cluster_dist = np.min([np.mean(distances[i][labels != label]) for label in set(labels) if label != labels[i]])\n",
    "        \n",
    "        # Calculate the Silhouette score for the current data point\n",
    "        silhouettes[i] = (other_cluster_dist - intra_cluster_dist) / max(intra_cluster_dist, other_cluster_dist)\n",
    "    \n",
    "    # Calculate the overall Silhouette score\n",
    "    silhouette_score = np.mean(silhouettes)\n",
    "\n",
    "    return silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcKoXSwwcSc8"
   },
   "source": [
    "**Task 4**: (10 points) \n",
    "\n",
    "To perform the clustering process, we will deine a `perform_clustering` method. This method will initialize the initial cluster centers, find the final centroids using the `k_means_clustering` method defined above, and calculate the Silhouette score using the `cal_sil_score` function. After performing all these tasks, the method should return the Silhouette score.\n",
    "\n",
    "For your task: implement an initialization method that randomly choose k different points in the dataset X as initial centroids. (5 points)\n",
    "\n",
    "Also, please answer the following questions (5 points): \n",
    "\n",
    "Is the k_means_Clustering algorithm sensitive to initial centroids? Explain.  Also, apart from random initialization, what other options are there for initialization?  Describe another option (do not need to implement in code). (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OySDZqSYs4eu"
   },
   "outputs": [],
   "source": [
    "def perform_clustering(X, k, dist_metric='Euclidian'):\n",
    "  '''\n",
    "\n",
    "   Parameters:\n",
    "    X: input data, numpy array of shape (number of samples, number of features)\n",
    "    k: number of clusters\n",
    "    dist_metric: distance metric to use for clustering, default is 'Euclidian'\n",
    "\n",
    "  Returns:\n",
    "    new_centroids: a numpy array of shape (k, number of features) with updated centroids\n",
    "    score: Silhouette score for the current clustering\n",
    "\n",
    "  '''\n",
    "\n",
    "  n_samples, n_features = X.shape\n",
    "\n",
    "  #STUDENT CODE STARTS HERE\n",
    "  # Randomly initialize the initial centroids (you can use 'np.random.choice' inbuilt function)\n",
    "  initial_centroids = \n",
    "  #STUDENT CODE ENDS HERE\n",
    "\n",
    "  #Find the cluster centers using the k_means_clustering function defined above.\n",
    "  new_centroids, labels = k_means_clustering(X, k, initial_centroids,dist_metric, 20)\n",
    "\n",
    "  #Evaluate the quality of the clusters by calculating the Silhouette score using the cal_sil_score function defined above.\n",
    "  score = cal_sil_score(X,labels)\n",
    "\n",
    "  \n",
    "  return new_centroids, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvQH6d-WHwqN"
   },
   "outputs": [],
   "source": [
    "# ploting sil score for different k values\n",
    "def plotting_scores(k_set,sil_scores):\n",
    "  plt.plot(k_set, sil_scores)\n",
    "  plt.xlabel('# of clusters')\n",
    "  plt.ylabel('sil_scores')\n",
    "  plt.title('# of clusters vs sil_score')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip9_7MpTwS_-"
   },
   "source": [
    "Write the code to integrate all the steps and execute the clustering algorithm for k=2 and k=3. Observe and report the resulting Silhouette scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNMi0ovcw5bm"
   },
   "outputs": [],
   "source": [
    "# find the final cluster centers and sil_scores for your clusterings using 'perform_clustering' method defined above\n",
    "final_2_centroids, score_2_clusters = perform_clustering(X,2,20)\n",
    "final_3_centroids, score_3_clusters = perform_clustering(X,3,20)\n",
    "# print(score_2_clusters, score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfJI2KI5Ub9W"
   },
   "source": [
    "**Task 5**: (5 points)\n",
    "\n",
    "As it's often difficult to determine the appropriate number of clusters in our data, we will conduct a grid search for k ranging from 2 to 10 clusters. Afterward, we'll plot the Silhouette scores against k. Based on this, what is the optimal value for k? Use `plotting_scores` provided above to generate the plot and paste it to your solution PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "Dxbf6WuD0O7O",
    "outputId": "d7bb7562-b831-4bfe-b11c-a4c559124337"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "k_set=[2,3,4,5,6,7,8,9,10] # students would populate\n",
    "sil_scores = [] # to be populated with corresponding silhouette_scores\n",
    "#STUDENT CODE HERE:\n",
    "\n",
    "#Evaluate the Silhouette scores for different k values using the perform_Clustering method defined above.\n",
    "\n",
    "#Create a plot of the k values versus the Silhouette scores using the plotting_scores function.\n",
    "\n",
    "#STUDENT CODE ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmQ-8cJOSkEO"
   },
   "source": [
    "**Task 6**: (10 points)\n",
    "\n",
    "In practice, it has been noticed that the initial cluster centers can significantly influence the clustering outcomes. Try out various centroid initialization techniques and determine which method produces better results than random initialization. It's essential to rewrite the perform_clustering function with your preferred initialization method, while the remaining part of the function should remain the same. For this comparison, select a fixed number of clusters, k = 5.\n",
    "\n",
    "Describe your method. Please report the Silhouette scores, along with a visualization of the clusters based on the random initialization and your method for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YxV9lOeTXJ5"
   },
   "outputs": [],
   "source": [
    "def perform_clustering_new():\n",
    "  # new initialization method\n",
    "\n",
    "  # rest of the method remains same\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJmtimFVUEqE"
   },
   "source": [
    "## **Household power consumption pattern clustering**\n",
    "Now, let's move on to a real-world dataset. The `Household_power_consumption_hourly.csv` file contains data for 1457 days of active energy consumption every hour (in watt hours) in a household by various electrical equipment. The original dataset can be found [here.](https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "GESOtD3LqlQu",
    "outputId": "a759f3d6-4e25-4a16-e8ec-a5dd2b2fc0a9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('household_power_consumption_hourly.csv')\n",
    "df.T.plot(figsize=(13,8), legend=False, color='blue', alpha=0.02)\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('KilloWatts')\n",
    "plt.xticks(range(0, 24), range(0, 24))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x7d-WNpyG7j"
   },
   "source": [
    "**Task 7**: (10 points)\n",
    "\n",
    "Perform K means clustering using `perform_clustering` on this dataset (5 points) and report the optimal number of clusters/consumption patterns based on the Silhouette score (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "KPdk1hSEraHo",
    "outputId": "6bccfa8e-41ce-4e24-b2c1-71983b26b489"
   },
   "outputs": [],
   "source": [
    "\n",
    "sil_scores = []\n",
    "k_set = np.arange(2,11).astype(int)\n",
    "\n",
    "X = df.values.copy()\n",
    "#STUDENT CODE STARTS HERE: \n",
    "# Feature scaling: Scale the data using minmax scaler\n",
    "\n",
    "\n",
    "#evaluate the goodness of clusters by calculating silhouette score using the cal_sil_score function defined above\n",
    "\n",
    "# Plot the Silhouette score vs number of clusters\n",
    "\n",
    "#STUDENT CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck47BkEArX5V"
   },
   "source": [
    "**Task 8**: (5 points)\n",
    "\n",
    "Utilize the given code below to examine the various clustering outcomes obtained at different k values, and then report your observations. Are they reasonable? Why or why not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "qeMHf4eZr8c2",
    "outputId": "c7cb6160-2479-46b3-cad6-281ce587e532"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('household_power_consumption_hourly.csv')\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "cluster_found = kmeans.fit_predict(X)\n",
    "cluster_found_sr = pd.Series(cluster_found, name='cluster')\n",
    "df = df.set_index(cluster_found_sr, append=True )\n",
    "\n",
    "fig, ax= plt.subplots(1,1, figsize=(18,10))\n",
    "color_list = ['blue','red','green']\n",
    "cluster_values = sorted(df.index.get_level_values('cluster').unique())\n",
    "\n",
    "for cluster, color in zip(cluster_values, color_list):\n",
    "    df.xs(cluster, level=1).T.plot(\n",
    "        ax=ax, legend=False, alpha=0.01, color=color, label= f'Cluster {cluster}'\n",
    "        )\n",
    "    df.xs(cluster, level=1).median().plot(\n",
    "        ax=ax, color=color, alpha=0.9, ls='--'\n",
    "    )\n",
    "\n",
    "ax.set_xticks(range(0, 24), range(0, 24))\n",
    "ax.set_ylabel('kilowatts')\n",
    "ax.set_xlabel('hour')\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7FYacuB7MD_"
   },
   "source": [
    "In this assignment, we delved into the K-means clustering algorithm, which is an unsupervised method for grouping data into clusters. Our study of this algorithm uncovered the following crucial insights:\n",
    "\n",
    "K-means clustering is influenced by both the initial cluster configurations. The number of clusters must be determined beforehand. We used the Silhouette score to evaluate the quality of our clusters, but you can also explore other methods such as the Davies Bouldin index.\n",
    "\n",
    "It's important to keep in mind that there are other clustering techniques, including Hierarchical Class Clustering and Latent Class Clustering. If you're interested, take some time to learn about these approaches and compare them to K-means clustering.\n",
    "\n",
    "Credit: Hardeep\n",
    "\n",
    "Edited by Ming Jin"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
